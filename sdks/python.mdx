---
sidebarTitle: "Python"
title: "Olostep Python SDK"
icon: python
description: "Official Python SDK for programmatic web scraping and data extraction"
---

<Note>
  **PyPI Package**: [olostep](https://pypi.org/project/olostep/) |
  **Requirements**: Python 3.11+
</Note>

## Table of Contents

- [Breaking Changes in v1.0.0](#Ô∏è-breaking-changes-in-v100)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [API Reference](#api-reference)
- [Error Handling](#error-handling)
- [Advanced Features](#advanced-features)
- [Olostep via API vs SDK](#olostep-via-api-vs-sdk)
- [Contributing](#contributing)
- [License](#license)

## ‚ö†Ô∏è Breaking Changes in v1.0.0

**Important**: Version 1.0.0 introduces breaking changes from v0.x.x. Please update your code accordingly.

Based on user feedback, release 1.0.0 will not only include more features but also provide a homogenized API that emulates patterns you find in other SDKs you might already be using. This introduces the following three types of breaking changes:

1. client class renaming,
2. and endpoint namespace standardization (plural)
3. endpoint method standardization (create)

### Client Classes

```python
# Change: The async client now uses the "Async" prefix much like other SDKs of similar design. We also took the opportunity to drop the "Client" part

# Old
from olostep import OlostepClient, SyncOlostepClient

# New
from olostep import AsyncOlostep, Olostep

# So
# OlostepClient -> AsyncOlostep and
# SyncOlostepClient -> Olostep
```

### Endpoint Namespaces

```python
# Change: All endpoint namespaces are now plural for consistency. Also, sitemap was renamed to maps.

# Old
result = (await) client.scrape.ENDPOINT
batch = (await) client.batch.ENDPOINT
crawl = (await) client.crawl.ENDPOINT
sitemap = (await) client.sitemap.ENDPOINT
content = (await) client.retrieve.ENDPOINT
answers = (await) client.answers.ENDPOINT

# New
result = (await) client.scrapes.ENDPOINT
batch = (await) client.batches.ENDPOINT
crawl = (await) client.crawls.ENDPOINT
maps = (await) client.maps.ENDPOINT
content = (await) client.retrieve.ENDPOINT
answers = (await) client.answers.ENDPOINT
```

### Endpoint Methods

```python
# Change: All creation methods now use .create() for consistency. Previously, batch and crawl used .start(). Scrape, answers, and sitemap/maps already used .create() and remain unchanged.

# Old
batch = (await) client.batch.start(urls)
crawl = (await) client.crawl.start(url)

# New
batch = (await) client.batches.create(urls)
crawl = (await) client.crawls.create(url)
```

## Installation

<CodeGroup>

```bash pip
pip install olostep
```

```bash pip3
pip3 install olostep
```

```bash poetry
poetry add olostep
```

```bash uv
uv pip install olostep
```

</CodeGroup>

### Requirements

- Python 3.11+
- API key from [olostep.com](https://www.olostep.com/?utm_source=python_sdk_readme)

## Quick Start

The SDK provides two client options depending on your use case:

| üìù Sync Client (`Olostep`)                                                                                                                                                                                                                                                                     | üöÄ Async Client (`AsyncOlostep`)                                                                                                                                                                                                                                                                                              |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Best for:** Scripts, beginners, and simple use cases where you prefer blocking operations.<br><br>The sync client provides a simpler, blocking interface that's easier to get started with if you're new to async/await.<br><br>üëâ **[See Sync Quick Start Guide](docs/quickstart_sync.md)** | **Best for:** Performance-critical applications, backend services, and handling many concurrent requests.<br><br>The async client provides non-blocking operations and is the recommended choice for production applications that need high throughput.<br><br>üëâ **[See Async Quick Start Guide](docs/quickstart_async.md)** |

## API Reference

### Method Structure

Both SDK clients provide the same clean, pythonic interface organized into logical namespaces:

| Namespace  | Purpose               | Key Methods                     |
| ---------- | --------------------- | ------------------------------- |
| `scrapes`  | Single URL extraction | `create()`, `get()`             |
| `batches`  | Multi-URL processing  | `create()`, `info()`, `items()` |
| `crawls`   | Website traversal     | `create()`, `info()`, `pages()` |
| `maps`     | Link extraction       | `create()`, `urls()`            |
| `answers`  | AI-powered extraction | `create()`, `get()`             |
| `retrieve` | Content retrieval     | `get()`                         |

Each operation returns stateful objects with ergonomic methods for follow-up operations.

## Error Handling

Catch all SDK errors using the base exception class:

```python
from olostep import Olostep, Olostep_BaseError

client = Olostep(api_key="your-api-key")

try:
    result = client.scrapes.create(url_to_scrape="https://example.com")
except Olostep_BaseError as e:
    print(f"Error has occurred: {type(e).__name__}")
    print(f"Error message: {e}")
```

For detailed error handling information, including the full exception hierarchy and granular error handling options, see [Error Handling](docs/error_handling.md).

### Automatic Retries

The SDK automatically retries on transient errors (network issues, temporary server problems) based on the `RetryStrategy` configuration. You can customize the retry behavior by passing a `RetryStrategy` instance when creating the client:

```python
from olostep import Olostep, RetryStrategy

retry_strategy = RetryStrategy(
    max_retries=3,
    initial_delay=1.0,
    jitter_min=0.2,
    jitter_max=0.8
)

client = Olostep(api_key="your-api-key", retry_strategy=retry_strategy)
result = client.scrapes.create("https://example.com")
```

For detailed retry configuration options and best practices, see [Retry Strategy](docs/retry_strategy.md).

## Advanced Features

### Smart Input Coercion

The SDK intelligently handles various input formats for maximum convenience:

```python
from olostep import Olostep, Country

client = Olostep(api_key="your-api-key")

# Formats: string, list, or enum
client.scrapes.create(url_to_scrape="https://example.com", formats="html")
client.scrapes.create(url_to_scrape="https://example.com", formats=["html", "markdown"])

# Countries: case-insensitive strings or enums
client.scrapes.create(url_to_scrape="https://example.com", country="us")
client.scrapes.create(url_to_scrape="https://example.com", country=Country.US)

# Lists: single values or lists
client.batches.create(urls="https://example.com")    # Single URL
client.batches.create(urls=["https://a.com", "https://b.com"])  # Multiple URLs
```

### Advanced Scraping Options

```python
from olostep import Olostep, Format, Country, WaitAction, FillInputAction

client = Olostep(api_key="your-api-key")

# Full control over scraping behavior
result = client.scrapes.create(
    url_to_scrape="https://news.google.com/",
    wait_before_scraping=3000,
    formats=[Format.HTML, Format.MARKDOWN],
    remove_css_selectors=["script", ".popup"],
    actions=[
        WaitAction(milliseconds=1500),
        FillInputAction(selector="searchbox", value="olostep")
    ],
    parser="@olostep/google-news",
    country=Country.US,
    remove_images=True
)
---
sidebarTitle: 'Python'
title: 'Olostep Python SDK'
icon: python
description: 'Official Python SDK for programmatic web scraping and data extraction'
---

<Note>
  **PyPI Package**: [olostep](https://pypi.org/project/olostep/) | **Requirements**: Python 3.11+
</Note>

## Installation

<CodeGroup>

```bash pip
pip install olostep
```

```bash pip3
pip3 install olostep
```

```bash poetry
poetry add olostep
```

```bash uv
uv pip install olostep
```

</CodeGroup>

## Authentication

Get your API key from [olostep.com/auth](https://www.olostep.com/auth)

<CodeGroup>

```python With API Key
from olostep import OlostepClient

client = OlostepClient(api_key="YOUR_API_KEY")
```

```bash Environment Variable
export OLOSTEP_API_KEY="YOUR_API_KEY"
```

```powershell Windows
$env:OLOSTEP_API_KEY="YOUR_API_KEY"
```

</CodeGroup>

## Quick Start

```python
"""
The quickstart uses the async/await interface as it's the default and generally preferred.
* If you need a blocking interface scroll to the end of this codeblock.
* If you want to see the full interfaces scroll to the next section.
"""

from olostep import OlostepClient

# Provide the API key either via passing in the 'api_key' parameter or
# by setting the OLOSTEP_API_KEY environment variable
client = OlostepClient(api_key="YOUR_REAL_KEY")


# MINIMAL SCRAPE EXAMPLE

scrape_result = await client.scrape("https://example.com")
# -> ScrapeResult(id='scrape_123', available=['html_content', 'markdown_content'])


# MINIMAL BATCH EXAMPLE

batch = await client.batch(["https://site1.com", "https://site2.com"])
# -> Batch(id='batch_123', urls=2)

# waits for all the batch jobs to finish, then starts fetching the results in batches
async for item in batch.items():
    content = await item.retrieve(["html"])
    print(f"{item.url}: {len(content.html_content)} bytes")



# MINIMAL CRAWL EXAMPLE

crawl = await client.crawl("https://example.com", max_pages=100)
# -> Crawl(id='crawl_123', urls=100)


async for page in crawl.pages():
    content = await page.retrieve(["html"])
    print(f"{page.url}: {len(content.html_content)} bytes")



# SYNC (FACADE) CLIENT
# this client is just a wrapper around the async client.
# The interface is the same, just don't use await.
# If you can use the OlostepClient instead, do so.
from olostep import SyncOlostepClient

client = SyncOlostepClient(api_key="YOUR_REAL_KEY")

scrape_result = client.scrape("https://example.com")
# -> ScrapeResult(id='scrape_123', available=['html_content', 'markdown_content'])
```

## Usage

The SDK provides a clean, Pythonic interface organized into logical namespaces. Each operation returns stateful objects with ergonomic methods for follow-up operations.

### Scraping
```python
from olostep import OlostepClient
from olostep import Country, FillInputAction, Format, LLMExtract, LinksOnPage, ScreenSize, Transformer, WaitAction

client = OlostepClient(api_key="YOUR_REAL_KEY")


# Minimal: Just scrape a URL
result = await client.scrape("https://example.com")
# ScrapeResult(id='scrape_123', available=['html_content', 'markdown_content'])

# Maximal: Full control over scraping behavior
result = await client.scrape(
    "https://example.com",
    wait_before_scraping=3000,
    formats=[Format.HTML, Format.MARKDOWN],
    remove_css_selectors=["script", ".popup"],
    actions=[
        WaitAction(milliseconds=1500),
        FillInputAction(selector="searchbox", value="olostep")
    ],
    country=Country.US,
    transformer=Transformer("postlight"),
    remove_images=True,
    remove_class_names=["ad"],
    parser="VALID_PARSER",  # check website for valid parsers
    llm_extract=LLMExtract(schema="YOUR_SCHEMA"),
    links_on_page=LinksOnPage(
        absolute_links=False,
        query_to_order_links_by='cars',
        include_links=["/events/**", "/offers/**"],
        exclude_links=[".pdf"]
    ),
    screen_size=ScreenSize(screen_width=1920, screen_height=1080),
    metadata={"custom": "sidecart_data"}  # Not supported yet
)
```

### Batch Processing
```python
from olostep import OlostepClient
from olostep import BatchItem, Country

client = OlostepClient(api_key="YOUR_REAL_KEY")


# Minimal: Process a list of URLs
batch = await client.batch(["https://site1.com", "https://site2.com"])
# Batch(id='batch_123', urls=2)

# Maximal: Advanced batch with custom IDs and options
batch = await client.batch(
    [
        BatchItem(url="https://www.google.com/search?q=olostep"),
        BatchItem(url="https://www.google.com/search?q=olostep+api", custom_id="news_2")
    ],
    country=Country.US,
    parser_id="@olostep/google-search"
)

# This is optional but you can check on the process of your batch at any time with:
info = await batch.info()
# -> BatchInfo(id='batch_123', status='in_progress', completed=1/2, age=2h ago)

# Also optional: Wait for completion.
# Pass in `check_every_n_secs=` to change interval, default 10
await batch.wait_till_done()


# Note: batch.items() automatically checks if the batch is completed before starting to return elements (can be disabled by passing in `wait_for_completion=False`)
async for item in batch.items(batch_size=10):
    content = await item.retrieve(["html", "json"])  # json from the parser
    print(f"{item.custom_id}: {len(content.html_content)} bytes")

# Alternative: Direct API access (stateless)
async for item in client.batch.items(batch_id='a_batch_id', batch_size=10):
    content = await item.retrieve(["html", "json"])
    print(f"{item.custom_id}: {len(content.html_content)} bytes")
```

### Web Crawling
```python
# Minimal: Crawl a site with default settings
crawl = await client.crawl("https://example.com", max_pages=100)
# Crawl(id='crawl_123', urls=100)

# Maximal: Advanced crawling with filters and limits
crawl = await client.crawl(
    "https://example.com",
    max_pages=1000,
    max_depth=3,
    include_urls=["/articles/**", "/news/**"],
    exclude_urls=["/ads/**", "/tracking/**"],
    include_external=False,
    include_subdomain=True,
    search_query="hot shingles",
    top_n=50
)

# This is optional but you can check on the process of your crawl at any time with:
info = await crawl.info()  # CrawlInfo(id='crawl_123', status='in_progress', pages_count=42, age=15m ago)

# Also optional: Wait for completion.
# Pass in `check_every_n_secs=` to change interval, default 10
await crawl.wait_till_done()

# Note: crawl.pages automatically checks if the batch is completed before starting to return elements (can be disabled by passing in `wait_for_completion=False`)
async for page in crawl.pages():
    content = await page.retrieve(["html"])
    print(f"{page.url}: {len(content.html_content)} bytes")

# Alternative: Direct API access (stateless)
async for page in client.crawl.pages(crawl_id='a_crawl_id'):
    content = await page.retrieve(["html"])
    print(f"{page.url}: {len(content.html_content)} bytes")
```

### Site Mapping
```python
# Minimal: Extract all links from a site
sitemap = await client.sitemap("https://example.com")
# Sitemap(id='map_123', urls_count=150, has_more=True)

# Maximal: Advanced link extraction with filters
sitemap = await client.sitemap(
    "https://example.com",
    search_query="documentation",
    top_n=500,
    include_subdomain=True,
    include_urls=["/docs/**", "/api/**"],
    exclude_urls=["/admin/**", "/private/**"]
)

# Seamless iteration over all URLs (auto-pagination)
all_urls = []
async for url in sitemap.urls():  # async generator
    print(f"Found URL: {url}")
    all_urls.append(url)
# Note: This can yield tens of thousands of URLs. If you can don't
#       create a list but use the generator as such.
```

### Data Retrieval
```python
# Notes:
#   * You should generally not need to use this endpoint as the other endpoints generate stateful return objects that can retrieve content.
#   * Not all formats are available all the time

# Minimal: Get content by retrieve ID
result = await client.retrieve("ret_123")
# ScrapeResult(id='ret_123', available=[...])

# Maximal: Get multiple formats
result = await client.retrieve("ret_123", ["html", "markdown", "text", "json"])
# ScrapeResult(id='ret_123', available=['html_content', 'markdown_content', 'text_content', 'json_content'])
```

## Advanced Features

### Method Shorthands

```python
# These are equivalent:
await client.scrape("https://example.com")            # shorthand
await client.scrape.create("https://example.com")     # explicit method

await client.batch(["url1", "url2"])                 # shorthand
await client.batch.start(["url1", "url2"])           # explicit method

await client.crawl("https://example.com")            # shorthand
await client.crawl.start("https://example.com")      # explicit method

await client.sitemap("https://example.com")          # shorthand
await client.sitemap.create("https://example.com")   # explicit method

await client.retrieve("ret_123")                     # shorthand
await client.retrieve.get("ret_123")                 # explicit method
```

### Smart Input Coercion

The SDK intelligently handles various input formats for maximum convenience:

```python
# Formats: string, list, or enum
await client.scrape("https://example.com", formats="html")
await client.scrape("https://example.com", formats=["html", "markdown"])


# Countries: case-insensitive strings or enums
await client.scrape("https://example.com", country="us")
await client.scrape("https://example.com", country=Country.US)

# Lists: single values or lists
await client.batch("https://example.com")    # Single URL
await client.batch(["https://a.com", "https://b.com"])  # Multiple URLs
```

## Error Handling

### Exception Hierarchy

The SDK handles error detection for you and provides a comprehensive exception hierarchy:

```text
* Olostep_BaseError -------------------------------------- <- Catch base class for all errors
  x Olostep_APIConnectionError --------------------------- <- No connection to the API
  x OlostepServerError_BaseError ------------------------- <- Server-issued errors (still detected in client ofc)
    + OlostepServerError_TemporaryIssue
      - OlostepServerError_NetworkBusy
      - OlostepServerError_InternalNetworkIssue
    + OlostepServerError_RequestUnprocessable
      - OlostepServerError_ParserNotFound
      - OlostepServerError_OutOfResources
    + OlostepServerError_BlacklistedDomain
    + OlostepServerError_FeatureApprovalRequired
    + OlostepServerError_AuthFailed
    + OlostepServerError_CreditsExhausted
    + OlostepServerError_InvalidEndpointCalled
    + OlostepServerError_ResourceNotFound
    + OlostepServerError_NoResultInResponse
    + OlostepServerError_UnknownIssue
  x OlostepClientError_BaseError ------------------------- <- Client-issued errors
    + OlostepClientError_RequestValidationFailed
    + OlostepClientError_ResponseValidationFailed
    + OlostepClientError_NoAPIKey
    + OlostepClientError_AsyncContext
    + OlostepClientError_BetaFeatureAccessRequired
    + OlostepClientError_Timeout
```

### Handling Errors

```python
from olostep import OlostepClient
from olostep.errors import (
    Olostep_BaseError,
    Olostep_APIConnectionError,
    OlostepServerError_AuthFailed,
    OlostepClientError_Timeout,
)

client = OlostepClient()

try:
    result = await client.scrape("https://example.com")
    content = await result.retrieve(["html"])
    print(content.html_content)
    
except Olostep_APIConnectionError:
    print("Network error - check your connection")
    
except OlostepServerError_AuthFailed:
    print("Authentication failed - check your API key")
    
except OlostepClientError_Timeout:
    print("Request timed out - try again")
    
except Olostep_BaseError as e:
    print(f"Olostep error: {e}")
```

<Note>
  The SDK automatically retries failed requests with exponential backoff for transient errors.
</Note>

## Logging

Enable logging to debug issues:

```python
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("olostep")
logger.setLevel(logging.INFO)  # Use DEBUG for verbose output
```

**Log Levels**: `INFO` (recommended), `DEBUG` (verbose), `WARNING`, `ERROR`

## Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `OLOSTEP_API_KEY` | Your API key | Required |
| `OLOSTEP_BASE_API_URL` | API base URL | `https://api.olostep.com/v1` |
| `OLOSTEP_API_TIMEOUT` | Request timeout (seconds) | `150` |

## Resources

<CardGroup cols={2}>
  <Card title="API Reference" icon="book" href="/api-reference/scrapes/create">
    Complete API documentation
  </Card>
  <Card title="Use Cases" icon="lightbulb" href="/use-cases">
    Real-world examples
  </Card>
  <Card title="PyPI Package" icon="python" href="https://pypi.org/project/olostep/">
    View on PyPI
  </Card>
  <Card title="Get API Key" icon="key" href="https://www.olostep.com/auth">
    Sign up for free
  </Card>
</CardGroup>

```

### Batch Processing with Custom IDs

```python
from olostep import Olostep, Country

client = Olostep(api_key="your-api-key")

batch = client.batches.create([
    {"url": "https://www.google.com/search?q=python", "custom_id": "search_1"},
    {"url": "https://www.google.com/search?q=javascript", "custom_id": "search_2"},
    {"url": "https://www.google.com/search?q=typescript", "custom_id": "search_3"}
],
country=Country.US,
parser="@olostep/google-search"
)

# Process results by custom ID
# When using a parser, retrieve JSON content instead of HTML
for item in batch.items():
    if item.custom_id == "search_2":
        content = item.retrieve(["json"])
        print(f"Search result: {content.json_content}")
```

### Intelligent Crawling

```python
from olostep import Olostep

client = Olostep(api_key="your-api-key")

# Crawl with intelligent filtering
crawl = client.crawls.create(
    start_url="https://www.bbc.com",
    max_pages=1000,
    max_depth=3,
    include_urls=["/articles/**", "/news/**"],
    exclude_urls=["/ads/**", "/tracking/**"],
    include_external=False,
    include_subdomain=True,
)

for page in crawl.pages():
    content = page.retrieve(["html"])
    print(f"Crawled: {page.url}")
```

### Site Mapping with Filters

```python
from olostep import Olostep

client = Olostep(api_key="your-api-key")

# Extract all links with advanced filtering
maps = client.maps.create(
    url="https://www.bbc.com",
    include_subdomain=True,
    include_urls=["/articles/**", "/news/**"],
    exclude_urls=["/ads/**", "/tracking/**"]
)

# Get filtered URLs
urls = []
for url in maps.urls():
    urls.append(url)

print(f"Found {len(urls)} relevant URLs")
```

### Answers Retrieval

```python
from olostep import Olostep

client = Olostep(api_key="your-api-key")

# First create an answer
created_answer = client.answers.create(
    task="What is the main topic of https://example.com?"
)

# Then retrieve it using the ID
answer = client.answers.get(answer_id=created_answer.id)
print(f"Answer: {answer.answer}")
```

### Content Retrieval

```python
from olostep import Olostep

client = Olostep(api_key="your-api-key")

# Get content by retrieve ID
result = client.retrieve.get(retrieve_id="ret_123")

# Get multiple formats
result = client.retrieve.get(retrieve_id="ret_123", formats=["html", "markdown", "text", "json"])
```

## Olostep via API vs SDK

The Olostep Python SDK offers significant advantages over direct API usage:

| Feature              | SDK Advantage                                 | Direct API              |
| -------------------- | --------------------------------------------- | ----------------------- |
| **Interface**        | Discoverable dot-notation namespaces          | REST endpoints          |
| **Type Safety**      | Full Pydantic validation & type hints         | Manual validation       |
| **State Management** | Stateful return objects with rich `__repr__`s | Manual state tracking   |
| **Error Handling**   | Automatic retries & connection management     | Manual implementation   |
| **Pagination**       | Elegant `(async) for` loops                   | Manual cursor handling  |
| **Input Coercion**   | Smart type coercion & validation              | Manual data preparation |
| **Performance**      | Async-first with sync facade                  | HTTP request overhead   |

### Key Benefits

- **Developer Experience**: Type-safe, ergonomic Python API with intelligent input handling
- **Production Ready**: Automatic retries, connection pooling, and error recovery
- **AI-Optimized**: Clean, structured data extraction perfect for LLM applications
- **Enterprise Scale**: Handle 100K+ concurrent requests with robust error handling
- **Future-Proof**: Built on modern Python patterns (async/await, type hints, dataclasses)

### Getting Help

- [Full Documentation](https://docs.olostep.com)
- [Report Issues](https://github.com/olostep-api/olostep-py/issues)
- [Community Slack](https://join.slack.com/t/olostep-users/shared_invite/zt-2pn2ce0uu-~591qIdhAfJy~LXCWQS5UQ)
- [Support Email](mailto:info@olostep.com)

## Contributing

We love contributions! Here's how you can help improve the Olostep Python SDK:

### Development Setup

```bash
git clone https://github.com/olostep-api/olostep-py/issues.git
cd olostep-python
pip install -e ".[dev]"
```

### Running Tests

```bash
# Run all tests
pytest

# Run specific test categories
pytest tests/unit/

# DO NOT RUN THE API CONTRACT TEST!
```

### Code Style

```bash
# Format code
ruff format olostep/

# Lint code
ruff check olostep/
mypy olostep/
```

### Submitting Changes

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-enhancement`)
3. Make your changes with comprehensive tests
4. Run the full test suite
5. Update documentation for API changes
6. Submit a pull request with a clear description

### Guidelines

- Follow PEP 8 and existing code style
- Add type hints for new functions
- Write comprehensive tests for new functionality
- Update documentation for API changes
- Use conventional commit messages

## Logging

Enable logging to debug issues:

```python
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("olostep")
logger.setLevel(logging.INFO)  # Use DEBUG for verbose output
```

**Log Levels**: `INFO` (recommended), `DEBUG` (verbose), `WARNING`, `ERROR`

## Configuration

### Environment Variables

| Variable               | Description               | Default                      |
| ---------------------- | ------------------------- | ---------------------------- |
| `OLOSTEP_API_KEY`      | Your API key              | Required                     |
| `OLOSTEP_BASE_API_URL` | API base URL              | `https://api.olostep.com/v1` |
| `OLOSTEP_API_TIMEOUT`  | Request timeout (seconds) | `150`                        |

## Resources

<CardGroup cols={2}>
  <Card title="API Reference" icon="book" href="/api-reference/scrapes/create">
    Complete API documentation
  </Card>
  <Card title="Use Cases" icon="lightbulb" href="/use-cases">
    Real-world examples
  </Card>
  <Card
    title="PyPI Package"
    icon="python"
    href="https://pypi.org/project/olostep/"
  >
    View on PyPI
  </Card>
  <Card title="Get API Key" icon="key" href="https://www.olostep.com/auth">
    Sign up for free
  </Card>
</CardGroup>
