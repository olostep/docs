---
sidebarTitle: Get specific pages in Website
title: Crawl and Extract Content from Stripe's Blog Pages
description: Learn how to crawl and extract content from Stripe's blog posts after identifying the URLs.
---

## Overview

This guide will show you how to:

- Start a crawl specifically targeting Stripe's blog posts
- Monitor the crawl progress
- Retrieve and process the crawled content

## Crawling Stripe's Blog Pages

To crawl Stripe's blog pages, use the crawls endpoint with the blog URLs you extracted in the previous step. This will fetch the full HTML content of each page, which you can then process to extract the information you need.

<CodeGroup>
    ```python crawl_stripe_blog.py
    import requests
    import time
    import json
    from datetime import datetime

    # Configuration
    API_URL = 'https://api.olostep.com/v1'
    API_KEY = '<your_olostep_api_key>'
    HEADERS = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {API_KEY}'
    }

    # Record start time for crawl duration tracking
    crawl_start_time = time.time()
    
    print(f"[{datetime.now().strftime('%H:%M:%S')}] Starting Stripe blog crawl...")
    
    # Start a crawl focused on Stripe's engineering blog posts
    # You can adjust the patterns based on your specific interests
    payload = {
        "start_url": "https://stripe.com/blog",
        "include_urls": ["/blog/engineering/**"],  # Focus on engineering posts
        "max_pages": 25  # Limit to 25 pages for this example
    }

    # Start the crawl
    print("Starting crawl of Stripe's engineering blog posts...")
    response = requests.post(f'{API_URL}/crawls', headers=HEADERS, json=payload)
    data = response.json()
    crawl_id = data['id']
    print(f"Crawl started with ID: {crawl_id}")

    # Monitor crawl progress
    while True:
        status_response = requests.get(f'{API_URL}/crawls/{crawl_id}', headers=HEADERS)
        status_data = status_response.json()
        print(f"Crawl status: {status_data['status']} - Pages crawled: {status_data.get('pages_count', 0)}")
        
        if status_data['status'] == 'completed' or status_data['status'] == 'failed':
            break
            
        # Wait 5 seconds before checking again
        time.sleep(5)

    # Calculate and display crawl duration
    crawl_duration = time.time() - crawl_start_time
    print(f"[{datetime.now().strftime('%H:%M:%S')}] Crawl completed in {crawl_duration:.2f} seconds")
    ```
</CodeGroup>

## Retrieving and Processing Blog Content

Once the crawl is complete, you can retrieve the content of all crawled pages and process it for your specific needs.

<CodeGroup>
    ```python retrieve_blog_content.py
    # Continuing from the previous example
    
    # If crawl completed successfully, retrieve the pages
    if status_data['status'] == 'completed':
        print(f"\nCrawl completed! Retrieved {status_data['pages_count']} pages.")
        pages_response = requests.get(f'{API_URL}/crawls/{crawl_id}/pages', headers=HEADERS)
        pages_data = pages_response.json()
        
        # Save the full crawl results
        with open('stripe_blog_content.json', 'w') as f:
            json.dump(pages_data, f, indent=2)
        print(f"Full crawl results saved to stripe_blog_content.json")
        
        # Print a sample of the first page
        first_page = pages_data['pages'][0]
        print(f"\nSample from first page: {first_page['url']}")
        print(f"Title: {first_page['title']}")
        print(f"Text excerpt: {first_page['text'][:200]}...")
    else:
        print(f"Crawl failed with status: {status_data['status']}")
    ```
</CodeGroup>

### Example Response

```json
{
  "pages": [
    {
      "url": "https://stripe.com/blog/engineering",
      "title": "Engineering | Stripe Blog",
      "content": "<html>...</html>",
      "text": "Engineering | Stripe Blog\nStripe builds economic infrastructure for the internet...",
      "metadata": {
        "og:title": "Engineering | Stripe Blog",
        "og:description": "Read about Stripe's engineering work, from infrastructure to product development."
      }
    },
    {
      "url": "https://stripe.com/blog/using-ml-to-detect-and-respond-to-performance-degradations",
      "title": "Using ML to detect and respond to performance degradations | Stripe Blog",
      "content": "<html>...</html>",
      "text": "Using ML to detect and respond to performance degradations..."  
    }
    // Additional pages...
  ]
}
```

## Next Steps

Now that you've successfully crawled and extracted content from Stripe's blog, you can:

1. **Expand your crawl**: Modify the `include_urls` parameter to crawl other sections of Stripe's blog
2. **Implement regular updates**: Set up a scheduled job to periodically crawl for new content
3. **Perform deeper analysis**: Use NLP tools to extract insights from the blog content
4. **Build a search engine**: Create a searchable database of Stripe's blog content

Using Olostep's content crawling capabilities, you can build powerful tools for monitoring and analyzing any websites content strategy.