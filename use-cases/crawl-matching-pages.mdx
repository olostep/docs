---
sidebarTitle: Crawl matching pages in a site
title: Crawl matching pages in a site
description: Extract and convert specific pages of a website into clean markdown format - perfect for feeding into LLMs.
---

## Overview

Crawling specific pages from a website and converting them to markdown is particularly useful when you need to:
- Extract documentation for use with LLMs
- Create a knowledge base from specific sections of a website
- Generate clean, structured content from complex web pages
- Focus on particular URL patterns within a large website

## Crawling Specific Pages

To crawl specific pages from a website, you'll use the crawls endpoint to start a new crawl with URL pattern filters. This allows you to target only the pages you need while ignoring irrelevant content.

<CodeGroup>
    ```python crawl_specific_pages.py
    from concurrent.futures import ThreadPoolExecutor, as_completed
    import requests
    import time
    import json
    from datetime import datetime

    # Configuration
    API_URL = 'https://api.olostep.com/v1'
    API_KEY = '<your_olostep_api_key>'
    HEADERS = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {API_KEY}'
    }

    # Record start time for crawl duration tracking
    crawl_start_time = time.time()
    
    print(f"[{datetime.now().strftime('%H:%M:%S')}] Starting documentation crawl...")
    
    # Start a new crawl with specific URL patterns
    data = {
        "start_url": "https://docs.stripe.com/payments/checkout/",
        "max_pages": 100,
        "include_urls": ["/payments/checkout/**"] # Only include pages matching this pattern
    }
    
    # Start the crawl
    response = requests.post(f'{API_URL}/crawls', headers=HEADERS, json=data)
    crawl_id = response.json()['id']
    
    # Wait for the crawl to complete
    while True:
        response = requests.get(f'{API_URL}/crawls/{crawl_id}', headers=HEADERS)
        pages_count = response.json()['pages_count']
        print(f'Pages completed: {pages_count}')
        if response.json()['status'] == 'completed':
            break
        
        print("Still crawling... Waiting 5 seconds")
        time.sleep(5)
    
    # Calculate and display crawl duration
    crawl_duration = time.time() - crawl_start_time
    print(f"[{datetime.now().strftime('%H:%M:%S')}] Crawl completed in {crawl_duration:.2f} seconds")
    ```
</CodeGroup>

## Converting Pages to Markdown

Once the crawl is complete, you can retrieve the content of all crawled pages in markdown format, which is ideal for feeding into LLMs or creating documentation.

<CodeGroup>
    ```python get_markdown_content.py
    # Continuing from the previous example
    
    # Fetch crawled pages with their markdown content
    params = {
        "retrieve_id": "<page_retrieve_id>",
        "formats": json.dumps(["markdown"])
    }
    
    # Get all pages from the crawl
    response = requests.get(f'{API_URL}/crawls/{crawl_id}/pages', headers=HEADERS)
    pages = response.json()['pages']
    
    results = []
    total_pages = len(pages)
    
    # Process pages in parallel for efficiency
    with ThreadPoolExecutor(max_workers=20) as executor:
        # Create futures for content retrieval
        future_to_page = {}
        for page in pages:
            params = {
                "retrieve_id": page['retrieve_id'],
                "formats": json.dumps(["markdown"])
            }
            future = executor.submit(
                requests.get, 
                f"{API_URL}/retrieve", 
                headers=HEADERS, 
                params=params
            )
            future_to_page[future] = page
        
        # Process results as they complete
        for i, future in enumerate(as_completed(future_to_page), 1):
            page = future_to_page[future]
            url = page['url']
            print(f"Processing {i}/{total_pages}: {url}")
            
            try:
                response = future.result()
                content_data = response.json()
                if content_data and "markdown_content" in content_data:
                    results.append({
                        'url': url,
                        'markdown_content': content_data['markdown_content']
                    })
                    print(f"✓ Content retrieved for {url}")
                else:
                    print(f"⚠ No markdown content for {url}")
            except Exception as e:
                print(f"❌ Error retrieving content for {url}: {str(e)}")
    
    print(f"Found {len(results)} pages with markdown content")
    ```
</CodeGroup>

## Saving the Markdown Content

After retrieving the markdown content, you can save it to a file for further processing or use with LLMs.

<CodeGroup>
    ```python save_markdown.py
    # Continuing from the previous example
    
    # Save all markdown content to a single file
    output_file = "doc_markdown.md"
    
    with open(output_file, "w", encoding="utf-8") as f:
        for page in results:
            url = page['url']
            content = page['markdown_content']
            
            # Write page header and content
            f.write(f"## URL: {url}\n\n")  # Add URL as a heading
            f.write(f"{content}\n\n")       # Add page content
            f.write("---\n\n")              # Add separator between pages
            
            print(f"✓ Added content from {url}")

    print(f"\n✅ Process complete! All content has been saved to '{output_file}'")
    print(f"Total pages processed: {len(results)}")
    ```
</CodeGroup>

## Complete Example

Here's a complete example that combines all the steps above into a reusable class:

<CodeGroup>
    ```python complete_example.py
    from concurrent.futures import ThreadPoolExecutor, as_completed
    import requests
    import time
    import json
    from datetime import datetime

    class OlostepManager:
        def __init__(self, api_key):
            self.API_URL = 'https://api.olostep.com/v1'
            self.HEADERS = {
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {api_key}'
            }

        def start_crawl(self, start_url, max_pages=250, include_urls=None):
            """Start a new crawl and return the crawl ID"""
            data = {
                "start_url": start_url,
                "max_pages": max_pages,
                "include_urls": include_urls or [f"{start_url}/**"]
            }
            response = requests.post(f'{self.API_URL}/crawls', headers=self.HEADERS, json=data)
            return response.json()['id']

        def await_crawl_completion(self, crawl_id):
            """Wait for the crawl to complete"""
            while True:
                response = requests.get(f'{self.API_URL}/crawls/{crawl_id}', headers=self.HEADERS)
                pages_count = response.json()['pages_count']
                print(f'Pages completed: {pages_count}')
                if response.json()['status'] == 'completed':
                    break
                
                print("Still crawling... Waiting 5 seconds")
                time.sleep(5)

        def get_pages(self, crawl_id, formats, max_workers=20):
            """Get all crawled pages with the specified formats content"""
            # Get pages from crawl
            response = requests.get(f'{self.API_URL}/crawls/{crawl_id}/pages', headers=self.HEADERS)
            pages = response.json()['pages']
            
            results = []
            total_pages = len(pages)
            
            # Process pages in parallel
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Create futures for content retrieval
                future_to_page = {
                    executor.submit(self._retrieve_content, page['retrieve_id'], formats): page
                    for page in pages
                }
                
                # Process results as they complete
                for i, future in enumerate(as_completed(future_to_page), 1):
                    page = future_to_page[future]
                    url = page['url']
                    print(f"Processing {i}/{total_pages}: {url}")
                    
                    try:
                        content_data = future.result()
                        if content_data and "markdown_content" in content_data:
                            results.append({
                                'url': url,
                                'markdown_content': content_data['markdown_content']
                            })
                            print(f"✓ Content retrieved for {url}")
                        else:
                            print(f"⚠ No markdown content for {url}")
                    except Exception as e:
                        print(f"❌ Error retrieving content for {url}: {str(e)}")
            
            return results

        def _retrieve_content(self, retrieve_id, formats):
            """Retrieves content for a single page."""
            params = {
                "retrieve_id": retrieve_id,
                "formats": json.dumps(formats)
            }
            response = requests.get(f"{self.API_URL}/retrieve", headers=self.HEADERS, params=params)
            return response.json()

        def save_markdown_to_file(self, pages, output_file="doc_markdown.md"):
            """Save markdown content to a file"""
            with open(output_file, "w", encoding="utf-8") as f:
                for page in pages:
                    url = page['url']
                    content = page['markdown_content']
                    
                    # Write page header and content
                    f.write(f"## URL: {url}\n\n")
                    f.write(f"{content}\n\n")
                    f.write("---\n\n")
                    
                    print(f"✓ Added content from {url}")

            print(f"\n✅ Process complete! All content has been saved to '{output_file}'")
            print(f"Total pages processed: {len(pages)}")

    # Usage example
    def main():
        # Initialize with your API key
        olostep = OlostepManager(api_key='<your_olostep_api_key>')
        
        # Record start time
        crawl_start_time = time.time()
        
        print(f"[{datetime.now().strftime('%H:%M:%S')}] Starting documentation crawl...")
        
        # Start crawl with specific URL patterns
        crawl_id = olostep.start_crawl(
            start_url="https://docs.stripe.com/payments/checkout/",
            max_pages=100,
            include_urls=["/payments/checkout/**"]  # Only include pages matching this pattern
        )
        
        # Wait for completion
        olostep.await_crawl_completion(crawl_id)
        
        # Calculate duration
        crawl_duration = time.time() - crawl_start_time
        print(f"[{datetime.now().strftime('%H:%M:%S')}] Crawl completed in {crawl_duration:.2f} seconds")

        # Get markdown content
        pages = olostep.get_pages(crawl_id=crawl_id, formats=["markdown"])
        
        # Save to file
        olostep.save_markdown_to_file(pages)

    if __name__ == "__main__":
        main()
    ```
</CodeGroup>

## Use Cases

This approach is particularly valuable for:

1. **LLM Training and Context** - Extract documentation to provide as context for LLMs to answer product-specific questions
2. **Documentation Conversion** - Convert web-based documentation to markdown for offline reading or integration with other systems
3. **Knowledge Base Creation** - Build a structured knowledge base from specific sections of a website
4. **API Documentation Analysis** - Extract and analyze API documentation from developer portals
5. **Content Migration** - Move content from one platform to another while preserving structure and formatting

By focusing on specific URL patterns, you can extract exactly the content you need while ignoring irrelevant pages, making this approach highly efficient for targeted content extraction.